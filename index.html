<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rob Kras</title>
    <meta name="description" content="Rob Kras - Computer Science, Data Science, AI Solutions">
    <link rel="stylesheet" href="styles.css">
    <script src="script.js" defer></script>
</head>
<body>
    <header>
        <div class="container">
            <img src="face.png" alt="Robin Kras" class="profile-pic">
            <h1>Rob Kras</h1>
            <p class="tagline">Quietly working away...</p>
            
            <nav class="social-links">
                <a href="https://www.linkedin.com/in/rob-kras/" target="_blank" aria-label="LinkedIn">
                    LinkedIn
                </a>
                <a href="mailto:robkraseu@gmail.com" aria-label="Email">
                    Email
                </a>
                <a href="https://github.com/rbkrs" target="_blank" aria-label="GitHub">
                    GitHub
                </a>
                <a href="Robin_Kras_resume.pdf" download aria-label="Resume">
                    Resume
                </a>
                <a href="https://www.kaggle.com/robkraseu" target="_blank" aria-label="Kaggle">
                    Kaggle
                </a>
            </nav>
        </div>
    </header>

    <main>
        <section class="about">
            <div class="container">
                <h2>About</h2>
                <p>Hello! I'm Rob, a Computer Scientist passionate about the intersection of mathematics, algorithms, and programming. I specialize in Data Science and Artificial Intelligence, with a proven track record in machine learning competitions.</p>
                
                <div class="highlights">
                    <div class="highlight">
                        <h3>Education</h3>
                        <p><strong>MSc Computer Science</strong><br>
                        Leiden University (2024-2025)<br>
                        <em>Specialization: Data Science & AI</em><br>
                        Thesis: Cross-Modal Sound Symbolism in Vision-Language Models</p>
                        
                        <p><strong>BSc Computer Science</strong><br>
                        Vrije Universiteit Amsterdam (2020-2023)<br>
                        <em>Minor: Data Science</em></p>
                    </div>
                    
                    <div class="highlight">
                        <h3>Technical Stack</h3>
                        <p><strong>Languages:</strong> Python, C/C++, Scala, JavaScript<br>
                        <strong>ML/AI:</strong> TensorFlow, PyTorch, Scikit-learn, Pandas, NumPy<br>
                        <strong>Tools:</strong> Git, SQL, Jupyter, HuggingFace, Docker<br>
                        <strong>Specializations:</strong> Machine Learning, Deep Learning, NLP, Computer Vision</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="research">
            <div class="container">
                <h2>Research</h2>
                
                <article class="project">
                    <div class="project-header">
                        <h3>Multimodal Sound Symbolism</h3>
                        <span class="rank exceptional">Grade: 8/10</span>
                    </div>
                    <div class="project-versions">
                        <a href="https://github.com/rbkrs/multimodal-sound-symbolism" target="_blank" class="version">GitHub Repository</a>
                    </div>
                    <p class="project-description">
                        My master's graduation research project investigating cross-modal sound symbolism in vision-language models. 
                        This thesis explored how modern AI systems understand the relationship between sounds and visual elements, 
                        examining the phenomenon where certain sounds are consistently associated with specific visual properties across cultures. 
                        The research combined computational linguistics, computer vision, and cognitive science to analyze how multimodal AI models 
                        process and represent these sound-meaning associations.
                    </p>
                </article>
            </div>
        </section>

        <section class="projects">
            <div class="container">
                <h2>Machine Learning Portfolio</h2>
                <p class="section-intro">A curated collection of my competitive machine learning projects, showcasing technical growth and problem-solving evolution.</p>
                
                <div class="project-list">
                    <article class="project">
                        <div class="project-header">
                            <h3>Titanic Survival Prediction</h3>
                            <span class="rank">Rank 2,331 / 15,346</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/titanic.html" class="version">Version 1</a>
                            <a href="projects/kaggle-competitions/titanic-v2.html" class="version improved">Version 2</a>
                        </div>
                        <p class="project-description">My first experience with Kaggle, albeit a practice problem. Learned feature engineering fundamentals, handling missing data, and the power of XGBoost over basic logistic regression. Overall
                             this challenge sparked my interest in machine learning. Having a place to apply it in a competitive yet educational environment was invaluable. In the second iteration, I applied a weight
                                scaling technique to boost likely-to-be-saved passengers, improving my score significantly. Besides this, removing features that are redundant when the ship sinks or are highly correlated to another feature helped as well.
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3>Spaceship Titanic</h3>
                            <span class="rank">Rank 613 / 1,816</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/titanic-spaceship.html" class="version">Version 1</a>
                            <a href="projects/kaggle-competitions/titanic-spaceship-v2.html" class="version improved">Version 2</a>
                        </div>
                        <p class="project-description">This was my second experience with Kaggle, building on the practice problem of the Titanic. 
                            In the first iteration I tried to create some new features myself, yet this didn't give me any favorable results.
                            After looking through the forums, I learned that removing related features as well as grid finetuning resulted in an increasingly higher performance.
                            Nevertheless, after looking through this problem again, it became apparent that the model I built was overfitting thanks to adding too many features. To counter this, I removed related features (or changed them into 1 feature), and reran the experiment.
                            From this, I achieved a higher score, and learned a valuable lesson about overfitting and feature engineering.    
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3>House Prices Prediction</h3>
                            <span class="rank top">Rank 37 / 3,935 (Top 1%)</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/house-prices.html" class="version">Version 1</a>
                            <a href="projects/kaggle-competitions/house-prices-v2.html" class="version improved">Version 2</a>
                            <a href="projects/kaggle-competitions/house-prices-v3.html" class="version exceptional">Version 3</a>
                        </div>
                        <p class="project-description">
                            The final practice problem I tackled before moving onto playground competitions! 
                            Here I had a major breakthrough in regression techniques. Discovered and learned from data leakage issues, balancing competitive performance with real-world applicability.
                            This is the first problem where I tried to incorporate domain knowledge into my feature engineering, which paid off well. By using SHAP, I was able to identify the most important features, and focus on those.
                            At the end, however, I made discovery that this competition had a data leakage issue, which I exploited to achieve an almost perfect score.
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3>Rainfall Prediction</h3>
                            <span class="rank top">Rank 5 / 2,529 (Top 0.2%)</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/rainfall.html" class="version">Version 1</a>
                            <a href="projects/kaggle-competitions/rainfall-v2.html" class="version exceptional">Version 2</a>
                        </div>
                        <p class="project-description">
                            In this competition I tried experimenting with a variety of techniques, including feature engineering, KFolds, and ensembling best public scores.
                            I learned that simpler algorithms (KNN) can outperform complex ensembles when properly optimized. Painful lesson about submission file formats.
                            Even worse, I also discovered that Kaggle wants you to select your best submission as the final one, rather than an ensemble of your best submissions. 
                            This sadly cost me a very high position, but taught me a valuable lesson about competition logistics. My highest achieved rank during this competition was 5th.
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3>Top 3 Fertilizer-types</h3>
                            <span class="rank">Rank 732 / 2,650</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/fertilizer.html" class="version">Version 1</a>
                        </div>
                        <p class="project-description">
                            This competition was about predicting the top 3 fertilizer types for a given crop and soil condition. 
                            I tried a variety of approaches, including advanced feature engineering, ensemble methods, and stacking techniques. 
                            Ultimately, I ended up with an approach where I create an ensemble of CATBoost, and XGBoost models, of which I then average out the results.
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3>Podcast Listening Behavior</h3>
                            <span class="rank">Rank 536 / 3,310</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/podcast.html" class="version">Version 1</a>
                        </div>
                        <p class="project-description">
                            In this competition I tried using a variety of advanced approaches, including meta modelling, KFolds, and finetuning on ensemble methods and advanced stacking techniques. 
                            This competition was about exploring user behavior with temporal features and listening patterns. I learned a lot about time series data and how to properly preprocess it for machine learning models.
                            Unfortunately, I made a submission file error which cost me a top 10% ranking, but I still learned a lot from the experience. The rank included on Kaggle is therefore not representative of my actual performance.
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3>Personality Type Prediction</h3>
                            <span class="rank">Rank 1,379 / 4,067</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/introvert.html" class="version">Version 1</a>
                        </div>
                        <p class="project-description">
                            This was a horrid competition about predicting personality types based on limited data. Since the data was too limited to train a proper model, I had to resort to advanced oversampling techniques like SMOTE to generate synthetic data points.
                            I also tried stacking multiple models to improve performance, and used Bayesian optimization to finetune hyperparameters. Sadly, the competition was not very well designed, and I learned that sometimes competitions are not worth the effort.
                            Similar to the previous problem, I forgot to select my best submission as the final one, which cost me a high ranking. Nevertheless, I learned a lot about handling imbalanced datasets and advanced ensemble techniques.
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3>Bank Customer Analysis</h3>
                            <span class="rank current">Rank 576 / 3,367</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/bank.html" class="version">Version 1</a>
                        </div>
                        <p class="project-description">
                            This competition was incredibly fun! It involved predicting whether a client will subscribe to a bank term deposit. 
                            By creating advanced features, applying stratified KFolds, using optuna, and creating an ensemble of models (lightGBM, XGBoost, CATBoost), 
                            I was able to achieve a high ranking, even after only participating in the first 7 days! Pretty cool.
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3>Credit Card Fraud Detector</h3>
                            <span class="rank">No rank, open dataset</span>
                        </div>
                        <div class="project-versions">
                            <a href="projects/kaggle-competitions/creditcard.html" class="version">Version 1</a>
                        </div>
                        <p class="project-description">
                            In this instance I decided to tackle a problem where the data is highly imbalanced and is related to real-world applications. Considering how cases of fraudulent activity are rare, 
                            actually training a model to find fraud poses a challenge for traditional machine learning algorithms. 
                            By using advanced oversampling techniques like SMOTE I was able to turn the imbalanced dataset into a balanced one, although I made the mistake of applying SMOTE before setting KFolds. 
                            I learned later on that it is always important to apply oversampling techniques within the training folds to avoid data leakage. 
                            This experience taught me a valuable lesson about using SMOTE correctly and to stay on my toes!
                        </p>
                    </article>

                    <article class="project">
                        <div class="project-header">
                            <h3 style="margin:0 auto;">Many more to come!</h3>
                        </div>
                    </article>
                </div>

                <div class="tools-section">
                    <h3>Open Source Contributions</h3>
                    <div class="tools">
                        <a href="https://github.com/rbkrs/utils" target="_blank" class="tool">
                            <strong>ML Utilities Library</strong>
                            <span>Reusable machine learning components and helper functions<br>
                            Collection of battle-tested utilities for data science workflows</span>
                        </a>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 Robin Kras</p>
            <p id="last-updated"></p>
        </div>
    </footer>
</body>
</html>